{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# Assignment 3 for Course 1MS041\n",
    "Make         sure you pass the `# ... Test` cells and\n",
    " submit your solution notebook in the corresponding assignment on the course website. You can submit multiple times before the deadline         and your highest score will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "3",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "8"
   },
   "source": [
    "---\n",
    "## Assignment 3, PROBLEM 1\n",
    "Maximum Points = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "3",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "8"
   },
   "source": [
    "\n",
    "Consider the data `X` and `y`, in the cell below. `X` denotes $20$ points in $\\mathbb{R}^2$ and `y` corresponds to the labels for these points, i.e. it is a classification problem.\n",
    "\n",
    "1. [3p] Implement the function `perceptron` by filling in `XXX`.\n",
    "2. [2p] Use your implemented `perceptron` function to compute a vector (numpy array) $\\hat w$ with shape `(3,1)` such that \n",
    "$$\n",
    "    (\\hat w \\cdot \\hat x_i) l_i > 0, \\quad \\forall i=1,\\ldots,20\n",
    "$$\n",
    "put your answer in `hat_w` below (the last dimension is the bias dimension, i.e. the added dimension we used to derive the perceptron)\n",
    "\n",
    "3. [3p] Use the vector $\\hat w$ that you just found and compute $r = \\max_i |x_i|$ (put your result in `r`), finally use this to give an upper bound to the number of iterations needed for the perceptron algorithm to converge on this dataset, see chapter 8 in the ITDS notes. Put the result in `iteration_bound`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "3",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "X = np.array([[0.14774693918368506,0.8537253157278155],[-0.1755517430286779,0.8979710703337818],[0.5227216475286975,0.7448281947022451],[-0.5071170511153492,0.8002027400836075],[-0.39436968212400453,1.0177689414422981],[-0.3983065780966649,1.0443663197782966],[-0.08652771617599643,0.48036820824519255],[0.15352541170101042,0.6820807981911706],[-0.3303348532791869,1.120673883903539],[-0.2656220857139274,0.8526638282828739],[0.7259603693529442,0.25428467532034965],[0.4577253912481767,-0.2358809079980879],[0.9722462145222105,0.13128550836973255],[0.4089349951770505,-0.09503914544452634],[0.9718156747909192,0.3524307824261209],[1.2009353774940565,-0.25004126389987974],[1.271791635779178,-0.07571928320750206],[0.36784476124502913,-0.23743021661715671],[0.8918396050420891,-0.1029336332277948],[0.4501578013678095,-0.13188266835015783]])+np.array([10,0]).reshape(1,-1)\n",
    "y = np.array([1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "3",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.49468903 34.08611021  3.        ] 11.316316542509355 78150.30109461867\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Part 1\n",
    "\n",
    "def perceptron(X_in, labels, max_iter=1000):\n",
    "    '''Runs the perceptron algorithm on X_in, labels, and does a maximum of max_iter iterations'''\n",
    "    # Adding the extra dimension with value 1 for the bias\n",
    "    X = np.c_[X_in, np.ones(X_in.shape[0])]\n",
    "    \n",
    "    # Initialize the weight vector w with zeros. Length is number of features + 1 (for bias)\n",
    "    w = np.zeros(X.shape[1])\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        for i in range(len(X)):\n",
    "            # Update the weight vector if a misclassification occurs\n",
    "            if (np.dot(w, X[i]) * labels[i]) <= 0:\n",
    "                w = w + labels[i] * X[i]\n",
    "    \n",
    "    return w  \n",
    "# Part 2\n",
    "hat_w= perceptron(X, y)\n",
    "\n",
    "# Part 3\n",
    "\n",
    "X_=np.c_[X, np.ones(X.shape[0])]\n",
    "r = np.max(np.linalg.norm(X_, axis=1))\n",
    "\n",
    "\n",
    "gamma=np.min(y*(np.dot(X_,hat_w)))\n",
    "positive_values = y * (np.dot(X_, hat_w))\n",
    "iteration_bound =(((r * np.linalg.norm(hat_w))**2 ) / (gamma)**2)\n",
    "print(hat_w,r,iteration_bound)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 给定的数据\n",
    "X = np.array([\n",
    "    [0.14774693918368506, 0.8537253157278155],\n",
    "    [-0.1755517430286779, 0.8979710703337818],\n",
    "    [0.5227216475286975, 0.7448281947022451],\n",
    "    [-0.5071170511153492, 0.8002027400836075],\n",
    "    [-0.39436968212400453, 1.0177689414422981],\n",
    "    [-0.3983065780966649, 1.0443663197782966],\n",
    "    [-0.08652771617599643, 0.48036820824519255],\n",
    "    [0.15352541170101042, 0.6820807981911706],\n",
    "    [-0.3303348532791869, 1.120673883903539],\n",
    "    [-0.2656220857139274, 0.8526638282828739],\n",
    "    [0.7259603693529442, 0.25428467532034965],\n",
    "    [0.4577253912481767, -0.2358809079980879],\n",
    "    [0.9722462145222105, 0.13128550836973255],\n",
    "    [0.4089349951770505, -0.09503914544452634],\n",
    "    [0.9718156747909192, 0.3524307824261209],\n",
    "    [1.2009353774940565, -0.25004126389987974],\n",
    "    [1.271791635779178, -0.07571928320750206],\n",
    "    [0.36784476124502913, -0.23743021661715671],\n",
    "    [0.8918396050420891, -0.1029336332277948],\n",
    "    [0.4501578013678095, -0.13188266835015783]\n",
    "]) + np.array([10, 0]).reshape(1, -1)\n",
    "\n",
    "y = np.array([1.0] * 10 + [-1.0] * 10)\n",
    "\n",
    "# Part 1\n",
    "def perceptron(X_in, labels, max_iter=1000):\n",
    "    '''运行感知机算法，输入为X_in和labels，最大迭代次数为max_iter'''\n",
    "    # 添加偏置项，值为1\n",
    "    X = np.c_[X_in, np.ones(X_in.shape[0])]\n",
    "    \n",
    "    # 初始化权重向量w为0，长度为特征数+1（偏置项）\n",
    "    w = np.zeros(X.shape[1])\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        for i in range(len(X)):\n",
    "            # 如果发生误分类，则更新权重向量\n",
    "            if (np.dot(w, X[i]) * labels[i]) <= 0:\n",
    "                w = w + labels[i] * X[i]\n",
    "    \n",
    "    return w  \n",
    "\n",
    "# Part 2\n",
    "hat_w = perceptron(X, y)\n",
    "\n",
    "# Part 3\n",
    "# 计算输入向量的最大范数\n",
    "r = np.max(np.linalg.norm(X, axis=1))\n",
    "\n",
    "# 计算gamma值\n",
    "gamma = np.min(y * (np.dot(np.c_[X, np.ones(X.shape[0])], hat_w)))\n",
    "\n",
    "# 计算迭代次数的上限\n",
    "iteration_bound = (r * np.linalg.norm(hat_w))**2 / gamma**2\n",
    "\n",
    "print(\"权重向量 (hat_w):\", hat_w)\n",
    "print(\"最大范数 (r):\", r)\n",
    "print(\"迭代次数上限:\", iteration_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "3",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "1",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "# # Part 1\n",
    "# hat_w = XXX\n",
    "# # Part 2\n",
    "# r = XXX\n",
    "# iteration_bound = XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "3",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "8"
   },
   "source": [
    "---\n",
    "## Assignment 3, PROBLEM 2\n",
    "Maximum Points = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "3",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "8"
   },
   "source": [
    "\n",
    "For this problem you will need the [pandas](https://pandas.pydata.org/) package and the [sklearn](https://scikit-learn.org/stable/) package. If you download the updated `data` folder from the course website you will find a file called `indoor_train.csv`, this file includes a bunch of positions in (X,Y,Z) and also a location number. The idea is to assign a room number (Location) to the coordinates (X,Y,Z).\n",
    "\n",
    "1. [2p] Take the data in the file `indoor_train.csv` and load it using pandas into a dataframe `df_train`\n",
    "2. [3p] From this dataframe `df_train`, create two numpy arrays, one `Xtrain` and `Ytrain`, they should have sizes `(1154,3)` and `(1154,)` respectively. Their `dtype` should be `float64` and `int64` respectively.\n",
    "3. [3p] Train a Support Vector Classifier, `sklearn.svc.SVC`, on `Xtrain, Ytrain` with `kernel='linear'` and name the trained model `svc_train`.\n",
    "\n",
    "To mimic how [kaggle](https://www.kaggle.com/) works, the Autograder has access to a hidden test-set and will test your fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "3",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "/Users/chenruizhen/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 1\n",
    "import pandas as pd\n",
    "df_train = pd.read_csv('data/indoor_train.csv')\n",
    "\n",
    "# Part 2\n",
    "Xtrain = df_train.loc[:,['Position X', ' Position Y', 'Position Z']].to_numpy(dtype=np.float64)\n",
    "Ytrain = df_train.loc[:,['Location']].to_numpy(dtype=np.int64)\n",
    "# Part 3\n",
    "from sklearn import svm\n",
    "\n",
    "svc_train = svm.SVC(kernel='linear')\n",
    "svc_train.fit(Xtrain, Ytrain)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "3",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "2",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "# df_train = XXX\n",
    "# Xtrain = XXX\n",
    "# Ytrain = XXX\n",
    "# svc_train = XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "3",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "8"
   },
   "source": [
    "---\n",
    "## Assignment 3, PROBLEM 3\n",
    "Maximum Points = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "3",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "8"
   },
   "source": [
    "\n",
    "## SMS spam filtering [8p]\n",
    "\n",
    "In the following problem we will explore SMS spam texts. The dataset is the `SMS Spam Collection Dataset` and we have provided for you a way to load the data. If you run the appropriate cell below, the result will be in the `spam_no_spam` variable. The result is a `list` of `tuples` with the first position in the tuple being the SMS text and the second being a flag `0 = not spam` and `1 = spam`.\n",
    "\n",
    "1. [3p] Let $X$ be the random variable that represents each SMS text (an entry in the list), and let $Y$ represent whether text is spam or not i.e. $Y \\in \\{0,1\\}$. Thus $\\mathbb{P}(Y = 1)$ is the probability that we get a spam. The goal is to estimate:\n",
    "$$\n",
    "    \\mathbb{P}(Y = 1 | \\text{\"free\" or \"prize\" is in } X) \\enspace .\n",
    "$$ \n",
    "That is, the probability that the SMS is spam given that \"free\" or \"prize\" occurs in the SMS. (This is precision). Hint: it is good to remove the upper/lower case of words so that we can also find \"Free\" and \"Prize\"; this can be done with `text.lower()` if `text` a string.\n",
    "\n",
    "2. [3p] Estimate the probability that the word \"free\" or \"prize\" is in the text given that it is spam. (This is recall) I.e. estimate\n",
    "$$\n",
    "    \\mathbb{P}(\\text{\"free\" or \"prize\" is in } X \\mid Y = 1) \\enspace .\n",
    "$$\n",
    "3. [2p] Provide a \"90\\%\" interval of confidence around the true probability from **part 1**. I.e. use the Hoeffding inequality to obtain for your estimate $\\hat P$. Find $l > 0$ such that the following holds:\n",
    "$$\n",
    "    \\mathbb{P}(\\hat P - l \\leq \\mathbb{E}[\\hat P] \\leq \\hat P + l) \\geq 0.9 \\enspace .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "3",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Run this cell to get the SMS text data\n",
    "def load_sms():\n",
    "    import csv\n",
    "    lines = []\n",
    "    hamspam = {'ham': 0, 'spam': 1}\n",
    "    with open('data/spam.csv', mode='r',encoding='latin-1') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        lines = [(line[1],hamspam[line[0]]) for line in reader]\n",
    "        \n",
    "    return lines\n",
    "spam_no_spam = load_sms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "3",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808695652173913\n",
      "0.07327138480158485\n"
     ]
    }
   ],
   "source": [
    "#part 1\n",
    "# fill in the estimate for part 1 here (should be a number between 0 and 1)\n",
    "import numpy as numpy\n",
    "\n",
    "def estimate_precision(data):\n",
    "    total_occurrences = sum('free' in text.lower() or 'prize' in text.lower() for text, label in data)\n",
    "    spam_occurrences = sum('free' in text.lower() or 'prize' in text.lower() for text, label in data if label == 1)\n",
    "\n",
    "    precision = spam_occurrences / total_occurrences\n",
    "\n",
    "    return precision\n",
    "    \n",
    "problem4_hatP = estimate_precision(spam_no_spam)\n",
    "print(problem4_hatP)\n",
    "\n",
    "# Part 2\n",
    "# Count the number of texts with \"free\" or \"prize\" given that they are spam\n",
    "# Divide by the total number of texts\n",
    "yfinx = 0\n",
    "fn=0\n",
    "for text, flag in spam_no_spam:\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    # Check if spam   \n",
    "    if flag == 1:\n",
    "    # Check if contains \"free\" or \"prize\"\n",
    "        if \"free\" in text or \"prize\" in text:\n",
    "            yfinx += 1        \n",
    "        else:\n",
    "            fn+=1\n",
    "# Divide by the number of spam texts\n",
    "problem4_hatP2 = yfinx / (yfinx+fn)\n",
    "\n",
    "#part3\n",
    "# fill in the calculated l from part 3 here\n",
    "alpha = 0.1\n",
    "n=sum('free' in text.lower() or 'prize' in text.lower() for text, label in spam_no_spam if label == 1)\n",
    "delta = (1 / np.sqrt(n)) * np.sqrt((1 / 2) * np.log(2 / alpha))\n",
    "problem4_l = delta\n",
    "\n",
    "print(problem4_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808695652173913 0.37349397590361444 0.07327138480158485\n"
     ]
    }
   ],
   "source": [
    "print(problem4_hatP,problem4_hatP2,problem4_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "lx_assignment_number": "3",
    "lx_assignment_type": "ASSIGNMENT",
    "lx_assignment_type2print": "Assignment",
    "lx_problem_cell_type": "PROBLEM",
    "lx_problem_number": "3",
    "lx_problem_points": "8"
   },
   "outputs": [],
   "source": [
    "# # fill in the estimate for part 2 here (should be a number between 0 and 1)\n",
    "# problem4_hatP2 = XXX\n",
    "# # fill in the calculated l from part 3 here\n",
    "# problem4_l = XXX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "lx_assignment_number": 3,
  "lx_course_instance": "2023",
  "lx_course_name": "Introduction to Data Science",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
